# Mini-GPT From Scratch

Implementation of a small GPT-style language model from scratch, following â€œLetâ€™s build GPT: from scratch, in codeâ€ by Andrej Karpathy.

## ğŸš€ What this repo does

- Implements tokenization, transformer blocks, attention mechanism and training loop in pure Python (with minimal dependencies)  
- Trains on a small text dataset to generate next-token predictions  
- Demonstrates core principles behind modern LLMs  

## ğŸ“¦ Project Structure

```
mini-gpt-from-scratch/
â”œâ”€â”€ gpt.ipynb         # Jupyter notebook with full implementation
â”œâ”€â”€ gpt.py            # (optional) plain-python script version
â”œâ”€â”€ data/             # (optional) dataset used for training
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â””â”€â”€ outputs/          # (optional) sample outputs, logs
```

## â–¶ï¸ How to Run

1. Clone this repo  
2. (Optional) create virtual environment and install requirements, e.g. `pip install torch numpy`  
3. Open `gpt.ipynb` in Jupyter Notebook and run all cells  

## ğŸ“š What youâ€™ll learn

- Transformer architecture basics: attention, multi-head attention, positional encoding  
- Tokenization & data preprocessing  
- Training and generation of text via next-token prediction  

## ğŸ§  Why this matters

Understanding how GPT works at a low level is critical for deeper study â€” including model evaluation, interpretability, security, and future custom LLM building.  

## ğŸ“ License

[MIT License](LICENSE) â€” feel free to use, modify and learn from this code.
